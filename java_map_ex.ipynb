{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "270fa6ce-147f-4966-92a5-f605f36afa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "// JAR 파일 경로를 변수로 출력한 후, 해당 경로를 매직 명령어에 사용\n",
    "%jars jars/*.jar // jars 디렉토리에 있는 모든 JAR 파일 추가\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c63ddbf-5d13-4fb9-86aa-825b0a702694",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession;\n",
    "import org.apache.spark.sql.Encoders;\n",
    "import org.apache.spark.sql.Dataset;\n",
    "import org.apache.spark.sql.Encoder;\n",
    "import org.apache.spark.sql.types.StructType;\n",
    "import org.apache.spark.sql.types.DataTypes;\n",
    "import java.io.Serializable;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf67a9d0-0e5f-47ac-97c8-d6b886690f12",
   "metadata": {},
   "outputs": [
    {
     "ename": "CompilationException",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30mDataset<Usage> dsUsage = \u001b[0m\u001b[1m\u001b[30m\u001b[41mspark\u001b[0m\u001b[1m\u001b[30m.createDataset(data, usageEncoder);\u001b[0m",
      "\u001b[1m\u001b[31mcannot find symbol\u001b[0m",
      "\u001b[1m\u001b[31m  symbol:   variable spark\u001b[0m",
      ""
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.Encoder;\n",
    "import org.apache.commons.lang3.RandomStringUtils;\n",
    "import java.io.Serializable;\n",
    "import java.util.Random;\n",
    "import java.util.ArrayList;\n",
    "import java.util.List;\n",
    "\n",
    "// 자바빈으로 자바 클래스 생성\n",
    "public class Usage implements Serializable {\n",
    "    int uid; // user id\n",
    "    String uname; // username\n",
    "    int usage; // usage\n",
    "\n",
    "    public Usage() {\n",
    "    }\n",
    "\n",
    "    public Usage(int uid, String uname, int usage) {\n",
    "        this.uid = uid;\n",
    "        this.uname = uname;\n",
    "        this.usage = usage;\n",
    "    }\n",
    "\n",
    "    // JavaBean getters and setters\n",
    "    public int getUid() {\n",
    "        return this.uid;\n",
    "    }\n",
    "\n",
    "    public void setUid(int uid) {\n",
    "        this.uid = uid;\n",
    "    }\n",
    "\n",
    "    public String getUname() {\n",
    "        return this.uname;\n",
    "    }\n",
    "\n",
    "    public void setUname(String uname) {\n",
    "        this.uname = uname;\n",
    "    }\n",
    "\n",
    "    public int getUsage() {\n",
    "        return this.usage;\n",
    "    }\n",
    "\n",
    "    public void setUsage(int usage) {\n",
    "        this.usage = usage;\n",
    "    }\n",
    "\n",
    "    @Override\n",
    "    public String toString() {\n",
    "        return \"uid: \" + this.uid + \" uname: \" + this.uname + \" usage: \" + this.usage;\n",
    "    }\n",
    "}\n",
    "\n",
    "// 명시적 인코더 생성\n",
    "Encoder<Usage> usageEncoder = Encoders.bean(Usage.class);\n",
    "Random rand = new Random();\n",
    "rand.setSeed(42);\n",
    "List<Usage> data = new ArrayList<Usage>();\n",
    "\n",
    "// 자바 Usage 클래스의 1000개 인스턴스 생성\n",
    "for (int i = 0; i < 1000; i++) {\n",
    "    data.add(new Usage(i, \"user\" + RandomStringUtils.randomAlphanumeric(5), rand.nextInt(1000)));\n",
    "}\n",
    "\n",
    "// Usage 형태의 데이터세트 생성\n",
    "Dataset<Usage> dsUsage = spark.createDataset(data, usageEncoder);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "015f5bb1-eaa2-4315-8133-a49dced7b169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.Row;\n",
    "import org.apache.spark.sql.RowFactory;\n",
    "import org.apache.spark.sql.Encoders;\n",
    "import org.apache.spark.sql.Dataset;\n",
    "import org.apache.spark.sql.catalyst.encoders.RowEncoder;\n",
    "import org.apache.spark.sql.types.DataTypes;\n",
    "import org.apache.spark.sql.types.StructField;\n",
    "import org.apache.spark.sql.types.StructType;\n",
    "import org.apache.spark.api.java.function.MapFunction;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "918fab1c-ee0e-4e40-92c0-50870e34cea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "// src/main/java/UsageExample.java\n",
    "import org.apache.spark.sql.SparkSession;\n",
    "import org.apache.spark.sql.Encoders;\n",
    "import org.apache.spark.sql.Dataset;\n",
    "import org.apache.spark.api.java.function.MapFunction;\n",
    "import org.apache.commons.lang3.RandomStringUtils;\n",
    "import org.apache.spark.api.java.function.FilterFunction;\n",
    "\n",
    "import java.io.Serializable;\n",
    "import java.util.Random;\n",
    "import java.util.ArrayList;\n",
    "import java.util.List;\n",
    "\n",
    "public class Usage implements Serializable {\n",
    "    int uid; // user id\n",
    "    String uname; // username\n",
    "    int usage; // usage\n",
    "\n",
    "    public Usage() {\n",
    "    }\n",
    "\n",
    "    public Usage(int uid, String uname, int usage) {\n",
    "        this.uid = uid;\n",
    "        this.uname = uname;\n",
    "        this.usage = usage;\n",
    "    }\n",
    "\n",
    "    // JavaBean getters and setters\n",
    "    public int getUid() {\n",
    "        return this.uid;\n",
    "    }\n",
    "\n",
    "    public void setUid(int uid) {\n",
    "        this.uid = uid;\n",
    "    }\n",
    "\n",
    "    public String getUname() {\n",
    "        return this.uname;\n",
    "    }\n",
    "\n",
    "    public void setUname(String uname) {\n",
    "        this.uname = uname;\n",
    "    }\n",
    "\n",
    "    public int getUsage() {\n",
    "        return this.usage;\n",
    "    }\n",
    "\n",
    "    public void setUsage(int usage) {\n",
    "        this.usage = usage;\n",
    "    }\n",
    "\n",
    "    @Override\n",
    "    public String toString() {\n",
    "        return \"uid: \" + this.uid + \" uname: \" + this.uname + \" usage: \" + this.usage;\n",
    "    }\n",
    "\n",
    "    public static Dataset<Usage> createUsageDataset(SparkSession spark) {\n",
    "        // 명시적 인코더 생성\n",
    "        Encoder<Usage> usageEncoder = Encoders.bean(Usage.class);\n",
    "        Random rand = new Random();\n",
    "        rand.setSeed(42);\n",
    "        List<Usage> data = new ArrayList<Usage>();\n",
    "\n",
    "        // 자바 Usage 클래스의 1000개 인스턴스 생성\n",
    "        for (int i = 0; i < 1000; i++) {\n",
    "            data.add(new Usage(i, \"user\" + RandomStringUtils.randomAlphanumeric(5), rand.nextInt(1000)));\n",
    "        }\n",
    "\n",
    "        // Usage 형태의 데이터세트 생성\n",
    "        return spark.createDataset(data, usageEncoder);\n",
    "    }\n",
    "    public static double computeCostUsage(int usage) {\n",
    "        if (usage > 750) {\n",
    "            return (usage - 750) * 0.1 + 750 * 0.5;\n",
    "        } else {\n",
    "            return usage * 0.5;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "public class UsageExample {\n",
    "    public static void main(String[] args) {\n",
    "        // 스파크 세션 생성\n",
    "        SparkSession spark = SparkSession.builder()\n",
    "                .appName(\"Usage Example\")\n",
    "                .master(\"local[*]\")\n",
    "                .getOrCreate();\n",
    "\n",
    "        // Usage 데이터셋 생성\n",
    "        Dataset<Usage> dsUsage = Usage.createUsageDataset(spark);\n",
    "        dsUsage.show(5);\n",
    "\n",
    "        StructType schema = DataTypes.createStructType(new StructField[]{\n",
    "    DataTypes.createStructField(\"user_id\", DataTypes.IntegerType, false),\n",
    "    DataTypes.createStructField(\"username\", DataTypes.StringType, false),\n",
    "    DataTypes.createStructField(\"cost\", DataTypes.DoubleType, false)\n",
    "});\n",
    "\n",
    "\n",
    "        // 인라인 MapFunction 정의\n",
    "     Dataset<Double> transformedUsage = dsUsage\n",
    "            .filter((FilterFunction<Usage>) u -> u.getUsage() >= 500)\n",
    "            .map((MapFunction<Usage, Double>) u -> {\n",
    "                if (u.getUsage() > 750) {\n",
    "                    return (u.getUsage()-750) * 0.1 + 750 * 0.5;\n",
    "                } else {\n",
    "                    return u.getUsage() * 0.50;\n",
    "                }\n",
    "            }, Encoders.DOUBLE())\n",
    "            .map((MapFunction<Double, Double>) value -> value + 100\n",
    "                 , Encoders.DOUBLE());\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        // 결과를 출력\n",
    "        transformedUsage.show(5);\n",
    "\n",
    "        // 스파크 세션 종료\n",
    "        spark.stop();\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "70aaffdf-4f8e-4f55-baec-64071749b7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/27 19:22:50 INFO SparkContext: Running Spark version 3.5.1\n",
      "24/05/27 19:22:50 INFO SparkContext: OS info Windows 11, 10.0, amd64\n",
      "24/05/27 19:22:50 INFO SparkContext: Java version 11.0.20\n",
      "24/05/27 19:22:50 INFO ResourceUtils: ==============================================================\n",
      "24/05/27 19:22:50 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "24/05/27 19:22:50 INFO ResourceUtils: ==============================================================\n",
      "24/05/27 19:22:50 INFO SparkContext: Submitted application: Usage Example\n",
      "24/05/27 19:22:50 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "24/05/27 19:22:50 INFO ResourceProfile: Limiting resource is cpu\n",
      "24/05/27 19:22:50 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "24/05/27 19:22:50 INFO SecurityManager: Changing view acls to: LG\n",
      "24/05/27 19:22:50 INFO SecurityManager: Changing modify acls to: LG\n",
      "24/05/27 19:22:50 INFO SecurityManager: Changing view acls groups to: \n",
      "24/05/27 19:22:50 INFO SecurityManager: Changing modify acls groups to: \n",
      "24/05/27 19:22:50 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: LG; groups with view permissions: EMPTY; users with modify permissions: LG; groups with modify permissions: EMPTY\n",
      "24/05/27 19:22:50 INFO Utils: Successfully started service 'sparkDriver' on port 55917.\n",
      "24/05/27 19:22:50 INFO SparkEnv: Registering MapOutputTracker\n",
      "24/05/27 19:22:50 INFO SparkEnv: Registering BlockManagerMaster\n",
      "24/05/27 19:22:50 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "24/05/27 19:22:50 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "24/05/27 19:22:50 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "24/05/27 19:22:50 INFO DiskBlockManager: Created local directory at C:\\Users\\LG\\AppData\\Local\\Temp\\blockmgr-feaf84c5-840b-4a91-80a2-c79ce4bd5682\n",
      "24/05/27 19:22:50 INFO MemoryStore: MemoryStore started with capacity 1018.8 MiB\n",
      "24/05/27 19:22:50 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "24/05/27 19:22:50 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "24/05/27 19:22:50 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "24/05/27 19:22:50 INFO Executor: Starting executor ID driver on host localhost\n",
      "24/05/27 19:22:50 INFO Executor: OS info Windows 11, 10.0, amd64\n",
      "24/05/27 19:22:50 INFO Executor: Java version 11.0.20\n",
      "24/05/27 19:22:50 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
      "24/05/27 19:22:50 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@ef10f07 for default.\n",
      "24/05/27 19:22:51 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 55942.\n",
      "24/05/27 19:22:51 INFO NettyBlockTransferService: Server created on localhost:55942\n",
      "24/05/27 19:22:51 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "24/05/27 19:22:51 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, localhost, 55942, None)\n",
      "24/05/27 19:22:51 INFO BlockManagerMasterEndpoint: Registering block manager localhost:55942 with 1018.8 MiB RAM, BlockManagerId(driver, localhost, 55942, None)\n",
      "24/05/27 19:22:51 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, localhost, 55942, None)\n",
      "24/05/27 19:22:51 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, localhost, 55942, None)\n",
      "24/05/27 19:22:51 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "24/05/27 19:22:51 INFO SharedState: Warehouse path is 'file:/C:/jupyter/notebook/spark-warehouse'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-----+\n",
      "|uid|    uname|usage|\n",
      "+---+---------+-----+\n",
      "|  0|userYYfsS|  130|\n",
      "|  1|userczNjY|  763|\n",
      "|  2|useryE5ks|  248|\n",
      "|  3|user8d4SJ|  884|\n",
      "|  4|userFZgjR|  970|\n",
      "+---+---------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/27 19:22:51 INFO SparkContext: Starting job: show at $JShell$36Q.java:71\n",
      "24/05/27 19:22:51 INFO DAGScheduler: Got job 0 (show at $JShell$36Q.java:71) with 1 output partitions\n",
      "24/05/27 19:22:51 INFO DAGScheduler: Final stage: ResultStage 0 (show at $JShell$36Q.java:71)\n",
      "24/05/27 19:22:51 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/05/27 19:22:51 INFO DAGScheduler: Missing parents: List()\n",
      "24/05/27 19:22:51 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at show at $JShell$36Q.java:71), which has no missing parents\n",
      "24/05/27 19:22:51 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 13.7 KiB, free 1018.8 MiB)\n",
      "24/05/27 19:22:51 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.6 KiB, free 1018.8 MiB)\n",
      "24/05/27 19:22:51 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:55942 (size: 5.6 KiB, free: 1018.8 MiB)\n",
      "24/05/27 19:22:51 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585\n",
      "24/05/27 19:22:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at show at $JShell$36Q.java:71) (first 15 tasks are for partitions Vector(0))\n",
      "24/05/27 19:22:51 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "24/05/27 19:22:51 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (localhost, executor driver, partition 0, PROCESS_LOCAL, 24004 bytes) \n",
      "24/05/27 19:22:51 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "24/05/27 19:22:51 INFO CodeGenerator: Code generated in 41.0283 ms\n",
      "24/05/27 19:22:51 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1488 bytes result sent to driver\n",
      "24/05/27 19:22:51 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 106 ms on localhost (executor driver) (1/1)\n",
      "24/05/27 19:22:51 INFO DAGScheduler: ResultStage 0 (show at $JShell$36Q.java:71) finished in 0.303 s\n",
      "24/05/27 19:22:51 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/05/27 19:22:51 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "24/05/27 19:22:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "24/05/27 19:22:51 INFO DAGScheduler: Job 0 finished: show at $JShell$36Q.java:71, took 0.363921 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|476.3|\n",
      "|488.4|\n",
      "|497.0|\n",
      "|362.5|\n",
      "|352.5|\n",
      "+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/27 19:22:51 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
      "24/05/27 19:22:51 INFO SparkUI: Stopped Spark web UI at http://localhost:4040\n",
      "24/05/27 19:22:51 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "24/05/27 19:22:51 INFO MemoryStore: MemoryStore cleared\n",
      "24/05/27 19:22:51 INFO BlockManager: BlockManager stopped\n",
      "24/05/27 19:22:51 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "24/05/27 19:22:51 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "24/05/27 19:22:51 INFO SparkContext: Successfully stopped SparkContext\n"
     ]
    }
   ],
   "source": [
    "UsageExample.main(new String[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb79b60b-c2a5-4657-8b2a-15ea49f2892b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset<Double> transformedUsage = dsUsage\n",
    "            .filter((FilterFunction<Usage>) u -> u.getUsage() >= 500)\n",
    "            .map((MapFunction<Usage, Double>) u -> {\n",
    "                if (u.getUsage() > 750) {\n",
    "                    return (u.getUsage()-750) * 0.1 + 750 * 0.5;\n",
    "                } else {\n",
    "                    return u.getUsage() * 0.50;\n",
    "                }\n",
    "            }, Encoders.DOUBLE());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "72c25394-5902-4ed5-865f-6b6b7cac034e",
   "metadata": {},
   "outputs": [
    {
     "ename": "CompilationException",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30mDataset<Double> transformedUsage = \u001b[0m\u001b[1m\u001b[30m\u001b[41mdsUsage\u001b[0m\u001b[1m\u001b[30m\u001b[0m",
      "\u001b[1m\u001b[31mcannot find symbol\u001b[0m",
      "\u001b[1m\u001b[31m  symbol:   variable dsUsage\u001b[0m",
      ""
     ]
    }
   ],
   "source": [
    "Dataset<Double> transformedUsage = dsUsage\n",
    "            .filter((FilterFunction<Usage>) u -> u.getUsage() >= 500)\n",
    "            .map((MapFunction<Usage, Double>) u -> {\n",
    "                if (u.getUsage() > 750) {\n",
    "                    return (u.getUsage()-750) * 0.1 + 750 * 0.5;\n",
    "                } else {\n",
    "                    return u.getUsage() * 0.50;\n",
    "                }\n",
    "            }, Encoders.DOUBLE())\n",
    "            .map((MapFunction<Double, Double>) value -> value + 100\n",
    "                 , Encoders.DOUBLE());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6fd435-0ef3-4756-b569-8cafae26a62e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Java",
   "language": "java",
   "name": "java"
  },
  "language_info": {
   "codemirror_mode": "java",
   "file_extension": ".jshell",
   "mimetype": "text/x-java-source",
   "name": "Java",
   "pygments_lexer": "java",
   "version": "11.0.20+9-LTS-256"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
