{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6baade0c-4655-4afc-929f-372b4abe0aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "// JAR 파일 경로를 변수로 출력한 후, 해당 경로를 매직 명령어에 사용\n",
    "%jars jars/*.jar // jars 디렉토리에 있는 모든 JAR 파일 추가\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b16d009-ae64-4576-ab69-be82170113a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "// 필요한 라이브러리 임포트\n",
    "import org.apache.spark.sql.SparkSession;\n",
    "import org.apache.spark.api.java.JavaRDD;\n",
    "import org.apache.spark.api.java.JavaSparkContext;\n",
    "import org.apache.spark.api.java.JavaPairRDD;\n",
    "import scala.Tuple2;\n",
    "\n",
    "public class SimpleSparkApp {\n",
    "    public static void main(String[] args) {\n",
    "        // SparkSession 생성\n",
    "        SparkSession spark = SparkSession.builder()\n",
    "                                         .appName(\"Simple Spark Application\")\n",
    "                                         .master(\"local[*]\")\n",
    "                                         .getOrCreate();\n",
    "\n",
    "        // JavaSparkContext 생성\n",
    "        JavaSparkContext sc = new JavaSparkContext(spark.sparkContext());\n",
    "\n",
    "        // 텍스트 파일 읽기\n",
    "        JavaRDD<String> lines = sc.textFile(\"textfile.txt\");\n",
    "\n",
    "        // 각 라인을 단어로 분리\n",
    "        JavaRDD<String> words = lines.flatMap(line -> Arrays.asList(line.split(\" \")).iterator());\n",
    "\n",
    "        // 단어를 (단어, 1) 형태로 매핑\n",
    "        JavaPairRDD<String, Integer> wordPairs = words.mapToPair(word -> new Tuple2<>(word, 1));\n",
    "\n",
    "        // 단어의 빈도 계산\n",
    "        JavaPairRDD<String, Integer> wordCounts = wordPairs.reduceByKey((a, b) -> a + b);\n",
    "\n",
    "        // 결과 출력\n",
    "        wordCounts.foreach(pair -> System.out.println(pair._1() + \" : \" + pair._2()));\n",
    "\n",
    "        // SparkSession 종료\n",
    "        spark.stop();\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6f9d7cb-b39c-4db0-9852-06d4c2890276",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/20 14:52:52 INFO SparkContext: Running Spark version 3.5.1\n",
      "24/05/20 14:52:52 INFO SparkContext: OS info Windows 11, 10.0, amd64\n",
      "24/05/20 14:52:52 INFO SparkContext: Java version 11.0.20\n",
      "24/05/20 14:52:52 INFO ResourceUtils: ==============================================================\n",
      "24/05/20 14:52:52 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "24/05/20 14:52:52 INFO ResourceUtils: ==============================================================\n",
      "24/05/20 14:52:52 INFO SparkContext: Submitted application: Simple Spark Application\n",
      "24/05/20 14:52:52 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "24/05/20 14:52:52 INFO ResourceProfile: Limiting resource is cpu\n",
      "24/05/20 14:52:52 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "24/05/20 14:52:52 INFO SecurityManager: Changing view acls to: LG\n",
      "24/05/20 14:52:52 INFO SecurityManager: Changing modify acls to: LG\n",
      "24/05/20 14:52:52 INFO SecurityManager: Changing view acls groups to: \n",
      "24/05/20 14:52:52 INFO SecurityManager: Changing modify acls groups to: \n",
      "24/05/20 14:52:52 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: LG; groups with view permissions: EMPTY; users with modify permissions: LG; groups with modify permissions: EMPTY\n",
      "24/05/20 14:52:53 INFO Utils: Successfully started service 'sparkDriver' on port 56363.\n",
      "24/05/20 14:52:53 INFO SparkEnv: Registering MapOutputTracker\n",
      "24/05/20 14:52:53 INFO SparkEnv: Registering BlockManagerMaster\n",
      "24/05/20 14:52:53 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "24/05/20 14:52:53 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "24/05/20 14:52:53 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "24/05/20 14:52:53 INFO DiskBlockManager: Created local directory at C:\\Users\\LG\\AppData\\Local\\Temp\\blockmgr-780d3271-e2f1-4fdf-98d3-0747137a16dd\n",
      "24/05/20 14:52:53 INFO MemoryStore: MemoryStore started with capacity 1018.8 MiB\n",
      "24/05/20 14:52:53 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "24/05/20 14:52:53 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "24/05/20 14:52:53 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "24/05/20 14:52:53 INFO Executor: Starting executor ID driver on host localhost\n",
      "24/05/20 14:52:53 INFO Executor: OS info Windows 11, 10.0, amd64\n",
      "24/05/20 14:52:53 INFO Executor: Java version 11.0.20\n",
      "24/05/20 14:52:53 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
      "24/05/20 14:52:53 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@75d2e045 for default.\n",
      "24/05/20 14:52:53 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 56386.\n",
      "24/05/20 14:52:53 INFO NettyBlockTransferService: Server created on localhost:56386\n",
      "24/05/20 14:52:53 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "24/05/20 14:52:53 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, localhost, 56386, None)\n",
      "24/05/20 14:52:53 INFO BlockManagerMasterEndpoint: Registering block manager localhost:56386 with 1018.8 MiB RAM, BlockManagerId(driver, localhost, 56386, None)\n",
      "24/05/20 14:52:53 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, localhost, 56386, None)\n",
      "24/05/20 14:52:53 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, localhost, 56386, None)\n",
      "24/05/20 14:52:53 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 221.5 KiB, free 1018.6 MiB)\n",
      "24/05/20 14:52:53 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.6 KiB, free 1018.6 MiB)\n",
      "24/05/20 14:52:53 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:56386 (size: 32.6 KiB, free: 1018.8 MiB)\n",
      "24/05/20 14:52:53 INFO SparkContext: Created broadcast 0 from textFile at $JShell$20B.java:35\n",
      "24/05/20 14:52:53 INFO FileInputFormat: Total input files to process : 1\n",
      "24/05/20 14:52:53 INFO SparkContext: Starting job: foreach at $JShell$20B.java:47\n",
      "24/05/20 14:52:53 INFO DAGScheduler: Registering RDD 3 (mapToPair at $JShell$20B.java:41) as input to shuffle 0\n",
      "24/05/20 14:52:53 INFO DAGScheduler: Got job 0 (foreach at $JShell$20B.java:47) with 2 output partitions\n",
      "24/05/20 14:52:53 INFO DAGScheduler: Final stage: ResultStage 1 (foreach at $JShell$20B.java:47)\n",
      "24/05/20 14:52:53 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)\n",
      "24/05/20 14:52:53 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)\n",
      "24/05/20 14:52:53 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at mapToPair at $JShell$20B.java:41), which has no missing parents\n",
      "24/05/20 14:52:53 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 7.8 KiB, free 1018.5 MiB)\n",
      "24/05/20 14:52:53 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.2 KiB, free 1018.5 MiB)\n",
      "24/05/20 14:52:53 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:56386 (size: 4.2 KiB, free: 1018.8 MiB)\n",
      "24/05/20 14:52:53 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585\n",
      "24/05/20 14:52:53 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at mapToPair at $JShell$20B.java:41) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/05/20 14:52:53 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks resource profile 0\n",
      "24/05/20 14:52:53 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (localhost, executor driver, partition 0, PROCESS_LOCAL, 7658 bytes) \n",
      "24/05/20 14:52:53 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (localhost, executor driver, partition 1, PROCESS_LOCAL, 7658 bytes) \n",
      "24/05/20 14:52:53 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "24/05/20 14:52:53 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)\n",
      "24/05/20 14:52:53 INFO HadoopRDD: Input split: file:/C:/jupyter/notebook/textfile.txt:0+67\n",
      "24/05/20 14:52:53 INFO HadoopRDD: Input split: file:/C:/jupyter/notebook/textfile.txt:67+67\n",
      "24/05/20 14:52:53 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1085 bytes result sent to driver\n",
      "24/05/20 14:52:54 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 151 ms on localhost (executor driver) (1/2)\n",
      "24/05/20 14:52:54 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1214 bytes result sent to driver\n",
      "24/05/20 14:52:54 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 202 ms on localhost (executor driver) (2/2)\n",
      "24/05/20 14:52:54 INFO DAGScheduler: ShuffleMapStage 0 (mapToPair at $JShell$20B.java:41) finished in 0.254 s\n",
      "24/05/20 14:52:54 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/05/20 14:52:54 INFO DAGScheduler: running: Set()\n",
      "24/05/20 14:52:54 INFO DAGScheduler: waiting: Set(ResultStage 1)\n",
      "24/05/20 14:52:54 INFO DAGScheduler: failed: Set()\n",
      "24/05/20 14:52:54 INFO DAGScheduler: Submitting ResultStage 1 (ShuffledRDD[4] at reduceByKey at $JShell$20B.java:44), which has no missing parents\n",
      "24/05/20 14:52:54 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 5.6 KiB, free 1018.5 MiB)\n",
      "24/05/20 14:52:54 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 3.1 KiB, free 1018.5 MiB)\n",
      "24/05/20 14:52:54 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "24/05/20 14:52:54 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:56386 (size: 3.1 KiB, free: 1018.8 MiB)\n",
      "24/05/20 14:52:54 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585\n",
      "24/05/20 14:52:54 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (ShuffledRDD[4] at reduceByKey at $JShell$20B.java:44) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/05/20 14:52:54 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks resource profile 0\n",
      "24/05/20 14:52:54 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2) (localhost, executor driver, partition 0, NODE_LOCAL, 7433 bytes) \n",
      "24/05/20 14:52:54 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3) (localhost, executor driver, partition 1, NODE_LOCAL, 7433 bytes) \n",
      "24/05/20 14:52:54 INFO Executor: Running task 0.0 in stage 1.0 (TID 2)\n",
      "24/05/20 14:52:54 INFO Executor: Running task 1.0 in stage 1.0 (TID 3)\n",
      "24/05/20 14:52:54 INFO ShuffleBlockFetcherIterator: Getting 1 (228.0 B) non-empty blocks including 1 (228.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/05/20 14:52:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms\n",
      "24/05/20 14:52:54 INFO ShuffleBlockFetcherIterator: Getting 1 (228.0 B) non-empty blocks including 1 (228.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/05/20 14:52:54 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark : 2\n",
      "Mesos, : 1\n",
      "a : 1\n",
      "on : 1\n",
      "or : 1\n",
      "is : 1\n",
      "Hello : 1\n",
      "runs : 1\n",
      "Apache : 2\n",
      "analytics : 1\n",
      "Kubernetes, : 1\n",
      "World : 1\n",
      "unified : 1\n",
      "engine : 1\n",
      "Hadoop, : 1\n",
      "in : 1\n",
      "cloud : 1\n",
      "standalone, : 1\n",
      "the : 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/20 14:52:54 INFO Executor: Finished task 0.0 in stage 1.0 (TID 2). 1752 bytes result sent to driver\n",
      "24/05/20 14:52:54 INFO Executor: Finished task 1.0 in stage 1.0 (TID 3). 1709 bytes result sent to driver\n",
      "24/05/20 14:52:54 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 272 ms on localhost (executor driver) (1/2)\n",
      "24/05/20 14:52:54 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 328 ms on localhost (executor driver) (2/2)\n",
      "24/05/20 14:52:54 INFO DAGScheduler: ResultStage 1 (foreach at $JShell$20B.java:47) finished in 0.372 s\n",
      "24/05/20 14:52:54 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/05/20 14:52:54 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "24/05/20 14:52:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
      "24/05/20 14:52:54 INFO DAGScheduler: Job 0 finished: foreach at $JShell$20B.java:47, took 0.827463 s\n",
      "24/05/20 14:52:54 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
      "24/05/20 14:52:54 INFO BlockManagerInfo: Removed broadcast_1_piece0 on localhost:56386 in memory (size: 4.2 KiB, free: 1018.8 MiB)\n",
      "24/05/20 14:52:54 INFO SparkUI: Stopped Spark web UI at http://localhost:4040\n",
      "24/05/20 14:52:54 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "24/05/20 14:52:54 INFO MemoryStore: MemoryStore cleared\n",
      "24/05/20 14:52:54 INFO BlockManager: BlockManager stopped\n",
      "24/05/20 14:52:54 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "24/05/20 14:52:54 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "24/05/20 14:52:54 INFO SparkContext: Successfully stopped SparkContext\n"
     ]
    }
   ],
   "source": [
    "SimpleSparkApp.main(new String[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec00153a-9393-43d7-a030-522211d4c47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession;\n",
    "import org.apache.spark.sql.Encoders;\n",
    "import org.apache.spark.sql.Dataset;\n",
    "import org.apache.spark.sql.Encoder;\n",
    "import org.apache.spark.sql.types.StructType;\n",
    "import org.apache.spark.sql.types.DataTypes;\n",
    "import java.io.Serializable;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52c3133b-76d8-4be8-b44b-56deaf0a493a",
   "metadata": {},
   "outputs": [],
   "source": [
    "public class Bloggers implements Serializable {\n",
    "    private int id;\n",
    "    private String first;\n",
    "    private String last;\n",
    "    private String url;\n",
    "    private String date;\n",
    "    private int hits;\n",
    "    private String[] campaigns;\n",
    "\n",
    "    // JavaBean getters and setters\n",
    "    int getID() { return id; }\n",
    "    void setID(int i) { id = i; }\n",
    "    String getFirst() { return first; }\n",
    "    void setFirst(String f) { first = f; }\n",
    "    String getLast() { return last; }\n",
    "    void setLast(String l) { last = l; }\n",
    "    String getURL() { return url; }\n",
    "    void setURL(String u) { url = u; }\n",
    "    String getDate() { return date; }\n",
    "    void setDate(String d) { date = d; }\n",
    "    int getHits() { return hits; }\n",
    "    void setHits(int h) { hits = h; }\n",
    "    String[] getCampaigns() { return campaigns; }\n",
    "    void setCampaigns(String[] c) { campaigns = c; }\n",
    "\n",
    "    public static void main(String[] args) {\n",
    "        // Create Spark session\n",
    "        SparkSession spark = SparkSession.builder()\n",
    "                                         .appName(\"Simple Spark Application\")\n",
    "                                         .master(\"local[*]\")\n",
    "                                         .getOrCreate();\n",
    "\n",
    "        // Create Encoder\n",
    "        Encoder<Bloggers> BloggerEncoder = Encoders.bean(Bloggers.class);\n",
    "        String bloggers = \"data_1.json\";\n",
    "        Dataset<Bloggers> bloggersDS = spark\n",
    "            .read()\n",
    "            .format(\"json\")\n",
    "            .option(\"path\", bloggers)\n",
    "            .option(\"multiline\", \"true\")\n",
    "            .load()\n",
    "            .as(BloggerEncoder);\n",
    "\n",
    "        // Show the dataset\n",
    "        bloggersDS.show(false);\n",
    "        \n",
    "        // Stop the Spark session\n",
    "        spark.stop();\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2699350-3926-4593-9163-ba73fb5bec87",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnresolvedReferenceException",
     "evalue": "Attempt to use definition snippet with unresolved references in Snippet:ClassKey(Bloggers)#12-public class Bloggers implements Serializable {\n    private int id;\n    private String first;\n    private String last;\n    private String url;\n    private String date;\n    private int hits;\n    private String[] campaigns;\n\n    // JavaBean getters and setters\n    int getID() { return id; }\n    void setID(int i) { id = i; }\n    String getFirst() { return first; }\n    void setFirst(String f) { first = f; }\n    String getLast() { return last; }\n    void setLast(String l) { last = l; }\n    String getURL() { return url; }\n    void setURL(String u) { url = u; }\n    String getDate() { return date; }\n    void setDate(String d) { date = d; }\n    int getHits() { return hits; }\n    void setHits(int h) { hits = h; }\n    String[] getCampaigns() { return campaigns; }\n    void setCampaigns(String[] c) { campaigns = c; }\n\n    public static void main(String[] args) {\n        // Create Spark session\n        SparkSession spark = SparkSession.builder()\n                                         .appName(\"Simple Spark Application\")\n                                         .master(\"local[*]\")\n                                         .getOrCreate();\n\n        // Create Encoder\n        Encoder<Bloggers> BloggerEncoder = Encoders.bean(Bloggers.class);\n        String bloggers = \"data_1.json\";\n        Dataset<Bloggers> bloggersDS = spark\n            .read()\n            .format(\"json\")\n            .option(\"path\", bloggers)\n            .option(\"multiline\", \"true\")\n            .load()\n            .as(BloggerEncoder);\n\n        // Show the dataset\n        bloggersDS.show(false);\n        \n        // Stop the Spark session\n        spark.stop();\n    }\n}",
     "output_type": "error",
     "traceback": [
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30mpublic class Bloggers implements Serializable {\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m    private int id;\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m    private String first;\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m    private String last;\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m    private String url;\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m    private String date;\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m    private int hits;\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m    private String[] campaigns;\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m    // JavaBean getters and setters\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m    int getID() { return id; }\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m    void setID(int i) { id = i; }\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m    String getFirst() { return first; }\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m    void setFirst(String f) { first = f; }\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m    String getLast() { return last; }\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m    void setLast(String l) { last = l; }\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m    String getURL() { return url; }\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m    void setURL(String u) { url = u; }\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m    String getDate() { return date; }\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m    void setDate(String d) { date = d; }\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m    int getHits() { return hits; }\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m    void setHits(int h) { hits = h; }\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m    String[] getCampaigns() { return campaigns; }\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m    void setCampaigns(String[] c) { campaigns = c; }\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m    public static void main(String[] args) {\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m        // Create Spark session\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m        SparkSession spark = SparkSession.builder()\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m                                         .appName(\"Simple Spark Application\")\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m                                         .master(\"local[*]\")\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m                                         .getOrCreate();\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m        // Create Encoder\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m        Encoder<Bloggers> BloggerEncoder = Encoders.bean(Bloggers.class);\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m        String bloggers = \"data_1.json\";\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m        Dataset<Bloggers> bloggersDS = spark\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m            .read()\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m            .format(\"json\")\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m            .option(\"path\", bloggers)\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m            .option(\"multiline\", \"true\")\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m            .load()\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m            .as(BloggerEncoder);\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m        // Show the dataset\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m        bloggersDS.show(false);\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m        \u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m        // Stop the Spark session\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m        spark.stop();\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m    }\u001b[0m",
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30m}\u001b[0m",
      "\u001b[1m\u001b[31mUnresolved dependencies:\u001b[0m",
      "\u001b[1m\u001b[31m   - class SparkSession\u001b[0m",
      "\u001b[1m\u001b[31m   - variable SparkSession\u001b[0m",
      "\u001b[1m\u001b[31m   - class Encoder\u001b[0m",
      "\u001b[1m\u001b[31m   - variable Encoders\u001b[0m",
      "\u001b[1m\u001b[31m   - class Dataset\u001b[0m"
     ]
    }
   ],
   "source": [
    "Bloggers.main(new String[0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a789afae-0548-4bad-879f-b5ca52e142c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession;\n",
    "import org.apache.spark.sql.Dataset;\n",
    "import org.apache.spark.sql.Encoder;\n",
    "import org.apache.spark.sql.Encoders;\n",
    "import java.io.Serializable;\n",
    "import java.util.ArrayList;\n",
    "import java.util.List;\n",
    "import java.util.Random;\n",
    "import org.apache.commons.lang3.RandomStringUtils;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3520b1de-37e5-4177-a3c0-ac4fb5620804",
   "metadata": {},
   "outputs": [],
   "source": [
    "public class Usage implements Serializable {\n",
    "    int uid;             // user id\n",
    "    String uname;        // username\n",
    "    int usage;           // usage\n",
    "\n",
    "    public Usage(int uid, String uname, int usage) {\n",
    "        this.uid = uid;\n",
    "        this.uname = uname;\n",
    "        this.usage = usage;\n",
    "    }\n",
    "\n",
    "    // JavaBean getters and setters\n",
    "    public int getUid() { return this.uid; }\n",
    "    public void setUid(int uid) { this.uid = uid; }\n",
    "    public String getUname() { return this.uname; }\n",
    "    public void setUname(String uname) { this.uname = uname; }\n",
    "    public int getUsage() { return this.usage; }\n",
    "    public void setUsage(int usage) { this.usage = usage; }\n",
    "\n",
    "    public Usage() { }\n",
    "\n",
    "    public String toString() {\n",
    "        return \"uid: '\" + this.uid + \"', uname: '\" + this.uname + \"', usage: '\" + this.usage + \"'\";\n",
    "    }\n",
    "\n",
    "    public static void main(String[] args) {\n",
    "        // Create Spark session\n",
    "        SparkSession spark = SparkSession.builder()\n",
    "                                         .appName(\"Usage Example\")\n",
    "                                         .master(\"local[*]\")\n",
    "                                         .getOrCreate();\n",
    "\n",
    "        // Create an explicit Encoder\n",
    "        Encoder<Usage> usageEncoder = Encoders.bean(Usage.class);\n",
    "        Random rand = new Random();\n",
    "        rand.setSeed(42);\n",
    "        List<Usage> data = new ArrayList<>();\n",
    "\n",
    "        // Create 1000 instances of Java Usage class\n",
    "        for (int i = 0; i < 1000; i++) {\n",
    "            data.add(new Usage(i, \"user\" + RandomStringUtils.randomAlphanumeric(5), rand.nextInt(1000)));\n",
    "        }\n",
    "\n",
    "        // Create a Dataset\n",
    "        Dataset<Usage> dsUsage = spark.createDataset(data, usageEncoder);\n",
    "\n",
    "        // Show the Dataset\n",
    "        dsUsage.show(false);\n",
    "\n",
    "        // Stop the Spark session\n",
    "        spark.stop();\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74c4e858-81c7-4992-94e7-219c03dc19e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/20 22:31:38 INFO SparkContext: Running Spark version 3.5.1\n",
      "24/05/20 22:31:39 INFO SparkContext: OS info Windows 11, 10.0, amd64\n",
      "24/05/20 22:31:39 INFO SparkContext: Java version 11.0.20\n",
      "24/05/20 22:31:39 INFO ResourceUtils: ==============================================================\n",
      "24/05/20 22:31:39 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "24/05/20 22:31:39 INFO ResourceUtils: ==============================================================\n",
      "24/05/20 22:31:39 INFO SparkContext: Submitted application: Usage Example\n",
      "24/05/20 22:31:39 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "24/05/20 22:31:39 INFO ResourceProfile: Limiting resource is cpu\n",
      "24/05/20 22:31:39 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "24/05/20 22:31:39 INFO SecurityManager: Changing view acls to: LG\n",
      "24/05/20 22:31:39 INFO SecurityManager: Changing modify acls to: LG\n",
      "24/05/20 22:31:39 INFO SecurityManager: Changing view acls groups to: \n",
      "24/05/20 22:31:39 INFO SecurityManager: Changing modify acls groups to: \n",
      "24/05/20 22:31:39 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: LG; groups with view permissions: EMPTY; users with modify permissions: LG; groups with modify permissions: EMPTY\n",
      "24/05/20 22:31:39 INFO Utils: Successfully started service 'sparkDriver' on port 53506.\n",
      "24/05/20 22:31:39 INFO SparkEnv: Registering MapOutputTracker\n",
      "24/05/20 22:31:39 INFO SparkEnv: Registering BlockManagerMaster\n",
      "24/05/20 22:31:39 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "24/05/20 22:31:39 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "24/05/20 22:31:39 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "24/05/20 22:31:39 INFO DiskBlockManager: Created local directory at C:\\Users\\LG\\AppData\\Local\\Temp\\blockmgr-7652734f-02a0-438e-bb65-e78fc0873b8c\n",
      "24/05/20 22:31:39 INFO MemoryStore: MemoryStore started with capacity 1018.8 MiB\n",
      "24/05/20 22:31:39 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "24/05/20 22:31:39 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "24/05/20 22:31:39 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "24/05/20 22:31:39 INFO Executor: Starting executor ID driver on host localhost\n",
      "24/05/20 22:31:39 INFO Executor: OS info Windows 11, 10.0, amd64\n",
      "24/05/20 22:31:39 INFO Executor: Java version 11.0.20\n",
      "24/05/20 22:31:39 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
      "24/05/20 22:31:39 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@4e1f60ea for default.\n",
      "24/05/20 22:31:40 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 53531.\n",
      "24/05/20 22:31:40 INFO NettyBlockTransferService: Server created on localhost:53531\n",
      "24/05/20 22:31:40 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "24/05/20 22:31:40 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, localhost, 53531, None)\n",
      "24/05/20 22:31:40 INFO BlockManagerMasterEndpoint: Registering block manager localhost:53531 with 1018.8 MiB RAM, BlockManagerId(driver, localhost, 53531, None)\n",
      "24/05/20 22:31:40 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, localhost, 53531, None)\n",
      "24/05/20 22:31:40 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, localhost, 53531, None)\n",
      "24/05/20 22:31:40 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "24/05/20 22:31:40 INFO SharedState: Warehouse path is 'file:/C:/jupyter/notebook/spark-warehouse'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-----+\n",
      "|uid|uname    |usage|\n",
      "+---+---------+-----+\n",
      "|0  |user9TDRf|130  |\n",
      "|1  |userYdY3W|763  |\n",
      "|2  |user9tDQS|248  |\n",
      "|3  |usernyGL6|884  |\n",
      "|4  |userc2HXs|970  |\n",
      "|5  |userwR2El|525  |\n",
      "|6  |useravb6d|505  |\n",
      "|7  |useryhSem|918  |\n",
      "|8  |userMYmvj|519  |\n",
      "|9  |userdWTlq|93   |\n",
      "|10 |userBkTJN|182  |\n",
      "|11 |userUKXl1|502  |\n",
      "|12 |userw5vir|276  |\n",
      "|13 |userW8Baj|292  |\n",
      "|14 |userciYkZ|476  |\n",
      "|15 |userKTluM|32   |\n",
      "|16 |userMp0h0|456  |\n",
      "|17 |userrpqAe|170  |\n",
      "|18 |userQa2yc|743  |\n",
      "|19 |userzivPD|209  |\n",
      "+---+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/20 22:31:40 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
      "24/05/20 22:31:40 INFO SparkUI: Stopped Spark web UI at http://localhost:4040\n",
      "24/05/20 22:31:40 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "24/05/20 22:31:40 INFO MemoryStore: MemoryStore cleared\n",
      "24/05/20 22:31:40 INFO BlockManager: BlockManager stopped\n",
      "24/05/20 22:31:40 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "24/05/20 22:31:40 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "24/05/20 22:31:40 INFO SparkContext: Successfully stopped SparkContext\n"
     ]
    }
   ],
   "source": [
    "Usage.main(new String[0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94456418-0e0d-4d62-8669-9275f9174383",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Java",
   "language": "java",
   "name": "java"
  },
  "language_info": {
   "codemirror_mode": "java",
   "file_extension": ".jshell",
   "mimetype": "text/x-java-source",
   "name": "Java",
   "pygments_lexer": "java",
   "version": "11.0.20+9-LTS-256"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
